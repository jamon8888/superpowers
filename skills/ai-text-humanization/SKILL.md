---
name: ai-text-humanization
description: Transform AI-generated text into natural, human-like writing using a systematic 10-phase assembly line process. Eliminates AI detection markers while preserving meaning and voice through sequential improvements targeting vocabulary, patterns, statistical signatures, dialogue, and strategic imperfections. Use when editing AI-generated content that sounds robotic or formulaic, removing purple prose and clichés, improving dialogue authenticity, optimizing text to pass AI detectors, or refining hybrid human-AI content. Based on research-backed detection countermeasures including nominalization conversion, n-gram filtering, punctuation variation, and statistical optimization.
---

# AI Text Humanization

Transform AI-generated text into natural, human-like writing through systematic processing that eliminates detection markers while preserving meaning and authenticity.

## When to Use This Skill

Use this skill when:
- Editing AI-generated content that sounds robotic or formulaic
- Removing AI-specific vocabulary, clichés, and purple prose
- Improving dialogue to sound more authentic and character-driven
- Optimizing text to reduce AI detection probability
- Refining content that mixes human and AI writing
- Processing creative writing, fiction, or narrative content
- Enhancing business content that feels too generic or corporate

**Do NOT use for:**
- Academic writing where AI use is prohibited
- Plagiarism or misrepresentation of authorship
- Content that must be verifiably human-written

## Core Principles

### The Assembly Line Approach

This skill uses **domain-specialized sequential processing** where each phase targets one specific aspect:

1. **Sequential processing** - Each phase builds on previous improvements
2. **Domain isolation** - No phase interferes with others' specialized work
3. **Pattern-based filtering** - Intelligent rules for common AI patterns
4. **Quality control** - Final sweep catches reintroduced AI markers
5. **Silent operation** - Focus on results, not commentary

### Key Philosophy

**Specialized Expertise Over Comprehensive Overhaul**
- Each phase excels at one improvement type
- Clear boundaries prevent conflicting changes
- Systematic approach ensures predictable, quality results

## The 10-Phase Humanization Process

Process text sequentially through all phases. Each phase takes the output from the previous phase as input.

### Phase 1: Grammar Foundation

**Purpose:** Establish clean grammatical base

**What it does:**
- Fixes grammar errors, typos, and basic mistakes
- Corrects sentence structure issues
- Ensures proper punctuation fundamentals
- Creates solid foundation for subsequent phases

**Key focus:** Technical correctness only - no style changes yet

---

### Phase 2: AI Word Cleaning

**Purpose:** Remove AI-specific vocabulary and patterns

**What it does:**
- Eliminates AI-associated terms and phrases
- Applies pattern-based filtering for common AI expressions
- Removes robotic or overly formal language
- Replaces generic business jargon

**Pattern rules target:**
- "It is important to note that..."
- "Plays a crucial role in..."
- "In order to..." (use "to" instead)
- "Leverage", "utilize", "implement" overuse
- "Delve", "realm", "landscape" (metaphorical misuse)

**Example transformations:**
- ❌ "It is important to note that the implementation leverages..."
- ✅ "The system uses..."

---

### Phase 3: Purple Prose Reduction + Nominalization Conversion

**Purpose:** Eliminate overwrought language and convert abstract constructions

**What it does:**
- Removes excessive descriptive flourishes
- Converts nominalized constructions to direct verbal forms
- Eliminates melodramatic or overwrought descriptions
- Simplifies unnecessarily complex phrasings

**Nominalization conversion (HIGH PRIORITY):**
- ❌ "The implementation of the solution..."
- ✅ "They implemented the solution..."
- ❌ "The consideration of the proposal..."
- ✅ "We considered the proposal..."

**Purple prose patterns:**
- "A symphony of..." → Direct description
- "Tapestry of..." → Simple listing
- "Echoing through..." → Direct statement
- "Weight of silence..." → "Silence"

---

### Phase 4: Sensory Enhancement

**Purpose:** Improve flat passages with concrete details

**What it does:**
- Adds specific sensory details where missing
- Replaces abstract descriptions with concrete ones
- Enhances setting and atmosphere naturally
- Keeps descriptions grounded and specific

**What to avoid:**
- Over-enhancement (don't add details everywhere)
- Clichéd sensory descriptions
- Disrupting narrative pacing

---

### Phase 5: Subtlety Creation

**Purpose:** Convert obvious statements into sophisticated implications

**What it does:**
- Replaces explicit statements with implication
- Shows rather than tells
- Removes redundant explanations
- Trusts reader intelligence

**Example transformations:**
- ❌ "She was angry, her face red with rage"
- ✅ "Her face flushed"
- ❌ "The room was cold and unwelcoming"
- ✅ "Frost edged the windows"

---

### Phase 6: Dialogue Enhancement

**Purpose:** Create authentic, character-specific voices

**What it does:**
- Enhances character voice distinctiveness
- Adds natural speech patterns (contractions, fragments, filler)
- Removes robotic or overly formal dialogue
- Preserves character-specific quirks

**Temperature setting:** Use temperature 1.0 for this phase for natural variation

**Dialogue patterns to eliminate:**
- "I must confess..." (overly formal)
- "It would seem..." (hedging)
- "Perhaps it would be wise..." (stilted)

**Dialogue patterns to add:**
- Natural contractions: "I'm", "don't", "won't"
- Sentence fragments: "Never mind." "Forget it."
- Character-specific quirks and verbal tics

---

### Phase 7: Weak Language Cleanup + Voice Distribution

**Purpose:** Strengthen language and balance active/passive voice

**What it does:**
- Removes weak modifiers (very, really, quite, just, actually)
- Replaces weak verbs with stronger alternatives
- Monitors active/passive voice distribution
- Targets 85-90% active voice for narrative

**Weak language categories (12 types):**
1. Hedging: "somewhat", "rather", "fairly"
2. Intensifiers without substance: "very", "extremely"
3. Vague modifiers: "things", "stuff", "aspects"
4. Weak verbs: "got", "made", "did"
5. Redundant phrases: "added bonus", "end result"
6. Abstract nouns: "situation", "scenario", "framework"
7. Passive constructions (excessive use)
8. Adverb overload: "quickly", "suddenly", "immediately"
9. Qualifying phrases: "I think", "I believe", "It seems"
10. Filler words: "basically", "essentially", "literally"
11. Weasel words: "up to", "as many as"
12. Empty emphasis: "really", "truly", "honestly"

**Voice distribution targets:**
- 85-90% active voice (narrative prose)
- 10-15% passive voice (variety, emphasis, flow)
- Avoid both extremes: 100% active sounds robotic, excessive passive sounds bureaucratic

---

### Phase 8: Strategic Imperfections

**Purpose:** Add natural rhythm and human inconsistency

**What it does:**
- Varies punctuation patterns (Oxford comma inconsistency)
- Injects logical leaps (removes over-explanation)
- Adds tangential thoughts (breaks perfect linearity)
- Retains some awkward phrasing (authentic voice)
- Creates sentence length variation (burstiness)

**Punctuation variation techniques:**
- Inconsistent Oxford comma use (sometimes yes, sometimes no)
- Varied spacing around em-dashes
- Mixed hyphenation patterns
- Occasional run-on sentences (where natural)

**Strategic imperfection types:**
- Logical leaps: Skip obvious connecting thoughts
- Tangential insertions: Brief off-topic moments
- Retained awkwardness: Keep some clunky-but-authentic phrases
- Minor inconsistencies: Varied formatting, punctuation

**What NOT to add:**
- Spelling errors (unless casual context demands it)
- Grammar mistakes
- Confusing constructions
- Anything that harms clarity

---

### Phase 9: AI Pattern Detection & Replacement

**Purpose:** Eliminate formulaic AI patterns and predictable collocations

**What it does:**
- Detects and replaces 150+ common AI n-grams (2-5 word sequences)
- Disrupts predictable collocations and formulaic expressions
- Identifies low-perplexity (overly predictable) constructions
- Replaces common business clichés

**Common AI n-gram patterns to eliminate:**
- "it is important to note that"
- "plays a crucial role in"
- "it is worth noting"
- "in today's world"
- "at the end of the day"
- "move the needle"
- "touch base"
- "circle back"
- "deep dive"
- "low-hanging fruit"

**Perplexity optimization:**
- Break predictable word pairs: "crystal clear" → "obvious"
- Disrupt formulaic phrases: "highly effective" → "powerful"
- Vary common collocations: "extensive research" → "thorough study"

**This phase focuses on QUALITATIVE pattern replacement**
- No statistical analysis here
- Pure pattern detection and substitution
- Statistical metrics handled separately in Phase 9.5

---

### Phase 9.5: Statistical Analysis Hub (OPTIONAL)

**Purpose:** Comprehensive statistical optimization in single pass

**When to use:**
- AI detection is a primary concern
- Text needs statistical balancing
- You want quantitative metrics

**When to skip:**
- Text already has natural statistical patterns
- Time/cost constraints
- Content is short-form or casual

**What it consolidates:**

**1. Burstiness (Sentence Variation)**
- Calculates coefficient of variation for sentence lengths
- Target: Mix of short (5-10 words), medium (15-20), long (25-35)
- Creates natural rhythm through length variation
- Human baseline: CV of 0.4-0.7

**2. POS Distribution (Part-of-Speech Ratios)**
- Normalizes noun/verb/adjective ratios to human baselines:
  - Nouns: 18-23%
  - Verbs: 16-20%
  - Adjectives: 6-9%
- Prevents noun-heavy (AI tendency) or verb-heavy (overcorrection) patterns

**3. Lexical Diversity (TTR - Type-Token Ratio)**
- Measures vocabulary richness
- Target: 0.40-0.60 TTR
- Optimizes word variety without forced synonyms
- Prevents both repetition and thesaurus abuse

**Single-pass efficiency:**
- Reads text once
- Calculates all metrics simultaneously
- Makes coordinated optimizations
- Provides optional detailed metrics report

**Optional output:** Before/after statistical scores showing improvements

---

### Phase 10: Final AI Word Sweep

**Purpose:** Quality control checkpoint for prohibited words

**What it does:**
- Pure word-level filtering (no statistical analysis)
- Catches AI-associated terms reintroduced by phases 3-9
- Final pass ensures no AI vocabulary slipped through
- Uses master prohibited words list

**This is NOT a catch-all phase**
- Focuses exclusively on word filtering
- Does not fix structural issues
- Does not recalculate statistics
- Clean separation from other concerns

**Common words caught in final sweep:**
- "delve", "realm", "landscape" (metaphorical misuse)
- "leverage", "utilize" (business jargon overuse)
- "underscoring its importance/significance"
- "plethora", "myriad" (overwrought vocabulary)

---

## Special Considerations

### ⚠️ Non-Native English Speaker (NNES) Bias Warning

AI detectors exhibit documented bias against NNES writing. NNES text often features:
- Simpler sentence structures
- Limited vocabulary range
- Reliance on common phrasings

**These characteristics naturally resemble AI patterns.**

**If processing NNES-authored content:**
- Be aware the full pipeline may over-simplify
- Consider selective application of phases
- Skip vocabulary expansion phases (2, 3)
- Focus on dialogue and voice (phases 6, 7)
- Test with small samples first

### ⚠️ Hybrid Text (Human + AI) Guidance

AI detectors often fail catastrophically on hybrid texts, misclassifying them as 100% human or 100% AI with no middle ground.

**This system assumes 100% AI-generated input.**

**For hybrid content:**
- Manually separate human-written sections from AI-generated
- Process ONLY AI sections through the pipeline
- Keep human-written content completely untouched
- Do NOT process mixed paragraphs together
- Consider whether detection is even a concern if substantial human contribution exists

## Workflow

### Sequential Processing Workflow

**For each phase:**

1. **Input text**: Use output from previous phase (or original text for Phase 1)
2. **Apply phase transformation**: Focus only on that phase's domain
3. **Verify domain isolation**: Ensure no interference with other phases' work
4. **Output for next phase**: Pass result to next phase

**Phase dependencies:**
- Phases 1-9 process sequentially
- Phase 9.5 is optional (skip if text already balanced)
- Phase 10 must always be final

### Practical Execution

**Manual Processing:**
```
Phase 1 → [output] → Phase 2 → [output] → Phase 3 → [output] → ...
→ Phase 9 → [output] → (Phase 9.5 optional) → [output] → Phase 10 → [final]
```

**Automated Processing:**
- Set up Claude Desktop Projects with custom instructions
- Use ChatGPT Projects for automated flow
- Integrate with n8n, Make.com, or API workflows
- Each phase passes output automatically to next

**Temperature settings:**
- Standard temperature for phases 1-5, 7-10
- **Temperature 1.0 for Phase 6** (dialogue natural variation)
- Phase 9.5: Standard temperature

### Quality Checks

After processing, verify:

**Phase Completion:**
- [ ] All 10 phases (or 11 with 9.5) completed sequentially
- [ ] No phases skipped accidentally
- [ ] Phase 6 used temperature 1.0
- [ ] Phase 10 caught any reintroduced AI terms

**Content Quality:**
- [ ] Meaning preserved from original
- [ ] Voice consistent and authentic
- [ ] Dialogue sounds natural
- [ ] Descriptions grounded and specific
- [ ] No purple prose or AI clichés remain
- [ ] Sentence rhythm feels natural

**Statistical Balance (if Phase 9.5 used):**
- [ ] Sentence lengths vary naturally
- [ ] POS ratios within human baselines
- [ ] Vocabulary diversity appropriate
- [ ] No forced synonyms or thesaurus abuse

**Final Verification:**
- [ ] Read entire piece aloud - does it sound human?
- [ ] Check for remaining AI patterns or formulaic language
- [ ] Verify character voices remain distinct (dialogue)
- [ ] Ensure no meaning was lost or distorted

## Common AI Patterns to Address

### Vocabulary Markers

**AI frequently uses:**
- "Delve", "realm", "landscape" (metaphorical misuse)
- "Leverage", "utilize", "implement" (business jargon overuse)
- "Plethora", "myriad", "crucial" (overwrought vocabulary)
- "Tapestry", "symphony", "echo" (purple prose metaphors)

**Replace with:**
- Direct, simple language
- Specific, concrete terms
- Natural vocabulary for context
- Varied word choice (but not forced)

### Structural Markers

**AI frequently creates:**
- Perfect Oxford comma consistency
- Zero punctuation variation
- Over-explanation of every logical step
- Perfectly linear progressions
- Excessively formal dialogue
- Abstract nominalized constructions

**Replace with:**
- Punctuation inconsistency (strategic)
- Logical leaps and tangential thoughts
- Natural, contracted dialogue
- Direct verbal constructions
- Sentence length variation

### Statistical Markers

**AI frequently exhibits:**
- Low burstiness (uniform sentence lengths)
- Noun-heavy POS distribution (18-23% baseline)
- Either very high or very low TTR
- Zero passive voice (overcorrection)
- Predictable n-gram patterns

**Balance toward:**
- High burstiness (varied sentence lengths)
- Human POS ratios (18-23% nouns, 16-20% verbs, 6-9% adjectives)
- 0.40-0.60 TTR
- 10-15% passive voice
- Disrupted predictable collocations

## Troubleshooting

**Problem:** Text sounds worse after processing
- **Solution:** Some phases may not suit your content type
- Skip phases 4-5 for non-narrative content
- Use only phases 1-3, 7-10 for business writing
- Test with small samples first

**Problem:** Dialogue became too casual
- **Solution:** Phase 6 temperature may be too high
- Adjust character formality in dialogue pass
- Verify temperature was 1.0 (not higher)

**Problem:** Meaning changed during processing
- **Solution:** Phase went beyond its domain
- Review each phase output individually
- Identify where meaning shifted
- Manually correct before proceeding

**Problem:** AI detection score didn't improve
- **Solution:** Statistical patterns may need optimization
- Ensure Phase 9.5 was included
- Verify all 10 phases completed
- Check for missed AI patterns in Phase 9

**Problem:** NNES text became over-simplified
- **Solution:** Full pipeline not appropriate for NNES content
- Use selective phases: 1, 6, 7, 8, 10 only
- Skip vocabulary/complexity phases
- Manual review between phases

## Quick Reference

**Phase 1:** Grammar foundation (errors only)
**Phase 2:** AI vocabulary removal (pattern-based)
**Phase 3:** Purple prose + nominalization conversion
**Phase 4:** Sensory enhancement (flat passages)
**Phase 5:** Subtlety creation (show don't tell)
**Phase 6:** Dialogue enhancement (temp 1.0)
**Phase 7:** Weak language + voice distribution
**Phase 8:** Strategic imperfections + rhythm
**Phase 9:** AI pattern detection + replacement
**Phase 9.5:** Statistical optimization (OPTIONAL)
**Phase 10:** Final prohibited word sweep

**Temperature:** Standard for all except Phase 6 (use 1.0)

**Special cases:**
- NNES content: Use phases selectively
- Hybrid text: Process AI and human sections separately
- Business writing: Skip phases 4-5
- Short-form: Skip Phase 9.5

**Key principle:** Each phase handles exactly one improvement type. Sequential processing ensures cumulative improvements without interference.

## Credits

Based on the ClaudeHumanizer system by pshort05, which employs research-backed detection countermeasures and systematic assembly line processing for AI text humanization.

**Repository:** https://github.com/pshort05/ClaudeHumanizer

## Reference Materials

This skill includes essential reference materials for AI text humanization:

### Core Files

- **master-prohibited-words.json**: **REQUIRED** for Phases 2 and 10. Contains 1,643 lines of AI vocabulary patterns, formulaic expressions, and pattern-matching rules. This file is essential for the automated detection and removal of AI markers.

### Documentation

- **docs/USAGE_GUIDE.md**: Complete step-by-step guide for processing text through all 10 phases, including troubleshooting, quality control, and automation setup instructions.

### How to Use

The `master-prohibited-words.json` file contains:
- Common AI vocabulary to eliminate
- N-gram patterns (2-5 word sequences)
- Purple prose metaphors
- Business jargon patterns
- Pattern-matching rules for dialogue, descriptions, and finger movements

**For manual processing**: Reference this file when applying Phases 2 and 10
**For automated processing**: Include this file in your API calls or automation workflows

See `docs/USAGE_GUIDE.md` for complete instructions on integrating these materials into your workflow.
